{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "slim = tf.contrib.slim \n",
    "from scipy.io import loadmat \n",
    "from scipy.misc import imread\n",
    "from scipy.misc import imresize\n",
    "from random import shuffle\n",
    "from datetime import timedelta\n",
    "from layers import * # bunch of wrapped layers that are we use to run the network\n",
    "from PIL import Image\n",
    "import from_image_to_floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load imdb dataset. Befor training the network you have to download dataset from the link below:\n",
    "# https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/\n",
    "# put images and .mat file mapping to folder /dataset/imdb_crom/\n",
    "# you might specify your own folder if you wish\n",
    "\n",
    "dataset_name = 'imdb'\n",
    "dataset_path = '../datasets/imdb_crop/imdb.mat'\n",
    "images_path = '../datasets/imdb_crop/'\n",
    "face_score_treshold = 3\n",
    "dataset = loadmat(dataset_path)\n",
    "image_names_array = dataset['imdb']['full_path'][0, 0][0]\n",
    "gender_classes = dataset['imdb']['gender'][0, 0][0]\n",
    "face_score = dataset['imdb']['face_score'][0, 0][0]\n",
    "second_face_score = dataset['imdb']['second_face_score'][0, 0][0]\n",
    "face_score_mask = face_score > face_score_treshold\n",
    "second_face_score_mask = np.isnan(second_face_score)\n",
    "unknown_gender_mask = np.logical_not(np.isnan(gender_classes))\n",
    "mask = np.logical_and(face_score_mask, second_face_score_mask)\n",
    "mask = np.logical_and(mask, unknown_gender_mask)\n",
    "image_names_array = image_names_array[mask]\n",
    "gender_classes = gender_classes[mask].tolist()\n",
    "image_names = []\n",
    "\n",
    "for image_name_arg in range(image_names_array.shape[0]):\n",
    "    image_name = image_names_array[image_name_arg][0]\n",
    "    image_names.append(image_name)\n",
    "    \n",
    "ground_truth_data = dict(zip(image_names, gender_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# splitting data to training and validation set\n",
    "\n",
    "def split_data(ground_truth_data, training_ratio=.8, do_shuffle=False):\n",
    "    ground_truth_keys = sorted(ground_truth_data.keys())\n",
    "    if do_shuffle == True:\n",
    "        shuffle(ground_truth_keys)\n",
    "    num_train = int(round(training_ratio * len(ground_truth_keys)))\n",
    "    train_keys = ground_truth_keys[:num_train]\n",
    "    validation_keys = ground_truth_keys[num_train:]\n",
    "    return train_keys, validation_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# before feeding labels to neural net we have to convert them to one-hot-encoded array\n",
    "\n",
    "def to_categorical(integer_classes, num_classes=3):\n",
    "    integer_classes = np.asarray(integer_classes, dtype='int')\n",
    "    num_samples = integer_classes.shape[0]\n",
    "    categorical = np.zeros((num_samples, num_classes))\n",
    "    categorical[np.arange(num_samples), integer_classes] = 1\n",
    "    return categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helping function that generate validation batch\n",
    "\n",
    "def generate_val_batch(j):\n",
    "    \n",
    "    test_batch_size = 256\n",
    "    x_batch = []\n",
    "    y_true_batch = [] \n",
    "    i = j\n",
    "    while len(y_true_batch) < test_batch_size and i < len(val_keys):\n",
    "        idx = i\n",
    "        image_path = images_path + val_keys[idx]\n",
    "        image_array = imread(image_path)\n",
    "        image_array = imresize(image_array, [img_size, img_size])\n",
    "        num_image_channels = len(image_array.shape)\n",
    "        if num_image_channels != 3:\n",
    "            i += 1\n",
    "            continue\n",
    "            \n",
    "        image_array = image_array.astype('float32')\n",
    "        x_batch.append(image_array)\n",
    "        \n",
    "        ground_truth = ground_truth_data[val_keys[idx]]\n",
    "        y_true_batch.append(ground_truth)\n",
    "        i += 1\n",
    "\n",
    "    \n",
    "    y_true_batch = to_categorical(y_true_batch, num_classes=2)\n",
    "    y_true_batch = np.asarray(y_true_batch, dtype='float32')\n",
    "    x_batch = np.asarray(x_batch, dtype='float32')\n",
    "\n",
    "    return x_batch, y_true_batch, i    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we might want to augment training data, this function slightly change input images\n",
    "# sometimes this allows to gain better accuracy\n",
    "# however we skip this part for simplicity\n",
    "\n",
    "def preprocess_image(image, training=True):\n",
    "    \n",
    "    #if training:\n",
    "    #    image = tf.image.random_flip_left_right(image)\n",
    "    #    \n",
    "    #    image = tf.image.random_hue(image, max_delta=0.05)\n",
    "    #    image = tf.image.random_contrast(image, lower=0.3, upper=1.0)\n",
    "    #    image = tf.image.random_brightness(image, max_delta=0.2)\n",
    "    #    image = tf.image.random_saturation(image, lower=0.0, upper=2.0)\n",
    "        \n",
    "    #    image = tf.minimum(image, 255.0)\n",
    "    #    image = tf.maximum(image, 0.0)\n",
    "    \n",
    "    return image    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wrapping function that helps to permute images from train batch\n",
    "def pre_process(images, training):\n",
    "    \n",
    "    images = tf.map_fn(lambda image: preprocess_image(image, training=training), images)\n",
    "    \n",
    "    return images       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate train batch\n",
    "\n",
    "def random_batch():\n",
    "    \n",
    "    x_batch = []\n",
    "    y_true_batch = []\n",
    "        \n",
    "    while len(x_batch) < train_batch_size:\n",
    "        idx = np.random.choice(num_images, replace=False)\n",
    "        image_path = images_path + train_keys[idx]\n",
    "        image_array = imread(image_path)\n",
    "        image_array = imresize(image_array, [img_size, img_size])\n",
    "        num_image_channels = len(image_array.shape)\n",
    "        if num_image_channels != 3:\n",
    "            continue\n",
    "            \n",
    "        image_array = image_array.astype('float32')       \n",
    "        x_batch.append(image_array)\n",
    "        \n",
    "        ground_truth = ground_truth_data[train_keys[idx]]\n",
    "        y_true_batch.append(ground_truth)\n",
    "    \n",
    "    y_true_batch = to_categorical(y_true_batch, num_classes=2)\n",
    "    y_true_batch = np.asarray(y_true_batch, dtype='float32')\n",
    "    x_batch = np.asarray(x_batch, dtype='float32')\n",
    "    \n",
    "    return x_batch, y_true_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_test_accuracy():\n",
    "    num_images = len(val_keys)\n",
    "    i = 0\n",
    "    acc_array = []\n",
    "    start_time = time.time()\n",
    "    while i < num_images:\n",
    "        x_batch, y_true_batch, k = generate_val_batch(i)\n",
    "        acc = session.run(accuracy, feed_dict={x: x_batch,\n",
    "                                      y_true: y_true_batch})\n",
    "        acc_array.append(acc)\n",
    "        i = k\n",
    "        msg = \"Checked {0:>6} pictures, Validation Accuracy of this block is: {1:>6.1%}\"\n",
    "        print(msg.format(i+1, acc))\n",
    "        \n",
    "    \n",
    "    total_acc = session.run(tf.reduce_mean(acc_array))\n",
    "    time_dif = time.time() - start_time\n",
    "    \n",
    "    print(\"Validation accuracy: {0:>6%}\".format(total_acc))\n",
    "    print(\"Time usage: \" + str(timedelta(seconds=int(round(time_dif)))))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_keys, val_keys = split_data(ground_truth_data)\n",
    "num_images = len(train_keys)\n",
    "train_batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_size = 48\n",
    "num_channels = 3\n",
    "num_classes = 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, img_size, img_size, num_channels], name='x')\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, num_classes], name='y_true')\n",
    "y_true_cls = tf.argmax(y_true, dimension=1, name='y_true_cls')\n",
    "    \n",
    "#48x48\n",
    "conv1_1 = conv_layer(inputs=x, W_shape=[5, 5, 3, 16], b_shape=16)\n",
    "conv1_1 = batch_norm(conv1_1)\n",
    "conv1_1 = tf.nn.relu(conv1_1, name='conv1_1')\n",
    "conv1_2 = conv_layer(conv1_1, W_shape=[5, 5, 16, 64], b_shape=64)\n",
    "conv1_2 = batch_norm(conv1_2)\n",
    "conv1_2 = tf.nn.relu(conv1_2, name='conv1_2')\n",
    "max_pool1 = max_pool(conv1_2)\n",
    "    \n",
    "#24x24\n",
    "conv2_1 = conv_layer(max_pool1, W_shape=[3, 3, 64, 128], b_shape=128)\n",
    "conv2_1 = slim.batch_norm(conv2_1)\n",
    "conv2_1 = tf.nn.relu(conv2_1, name='conv2_1')\n",
    "conv2_2 = conv_layer(conv2_1, W_shape=[3, 3, 128, 256], b_shape=256)\n",
    "conv2_2 = slim.batch_norm(conv2_2)\n",
    "conv2_2 = tf.nn.relu(conv2_2, name='conv2_2')\n",
    "max_pool2 = max_pool(conv2_2)\n",
    "    \n",
    "#12x12\n",
    "conv3_1 = conv_layer(max_pool2, W_shape=[3, 3, 256, 256], b_shape=256)\n",
    "conv3_1 = slim.batch_norm(conv3_1)\n",
    "conv3_1 = tf.nn.relu(conv3_1, name='conv3_1')\n",
    "\n",
    "conv3_2 = conv_layer(conv3_1, W_shape=[3, 3, 256, 512], b_shape=512)\n",
    "conv3_2 = slim.batch_norm(conv3_2)\n",
    "conv3_2 = tf.nn.relu(conv3_2, name='conv3_2')\n",
    "max_pool3 = max_pool(conv3_2)\n",
    "    \n",
    "#6x6\n",
    "flatten, num_features = flatten_layer(max_pool3)\n",
    "    \n",
    "#1x1\n",
    "fc1 = fc_layer(flatten, num_features, 256, name=\"fc_1\")\n",
    "fc2 = fc_layer(fc1, 256, num_classes, use_relu=False)\n",
    "y_pred = tf.nn.softmax(fc2)\n",
    "\n",
    "y_pred_cls = tf.argmax(y_pred, dimension=1, name=\"y_pred_cls\")\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=fc2,\n",
    "                                                        labels=y_true)\n",
    "cost = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cost)\n",
    "correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "session = tf.InteractiveSession()\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize(num_iterations):\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        x_batch, y_true_batch = random_batch()\n",
    "        \n",
    "        feed_dict_train = {x: x_batch,\n",
    "                           y_true: y_true_batch}\n",
    "        \n",
    "        session.run(optimizer, feed_dict_train)\n",
    "        if i % 100 == 0:\n",
    "            acc = session.run(accuracy, feed_dict=feed_dict_train)\n",
    "            msg = \"Optimization Iteration: {0:>6}, Training Accuracy: {1:>6.1%}\"\n",
    "            print(msg.format(i, acc))\n",
    "            \n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    print(\"Time usage: \" + str(timedelta(seconds=int(round(time_dif)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_dir = 'checkpoint/'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "    \n",
    "save_path = os.path.join(save_dir, 'gender_recognition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Iteration:      0, Training Accuracy:  60.9%\n",
      "Optimization Iteration:    100, Training Accuracy:  78.1%\n",
      "Optimization Iteration:    200, Training Accuracy:  78.1%\n",
      "Time usage: 0:34:19\n"
     ]
    }
   ],
   "source": [
    "optimize(num_iterations=201)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_layer_weights(name):\n",
    "    with tf.variable_scope('conv1_1', reuse=True) as scope_conv:\n",
    "        W_conv1 = tf.get_variable('weights', shape=[5, 5, 3, 16])\n",
    "        ww = session.run(W_conv1)\n",
    "        return ww"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoint/gender_recognition'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver.save(session, save_path=save_path)\n",
    "#saver.restore(sess=session, save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.framework.graph_util import convert_variables_to_constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 22 variables.\n",
      "Converted 22 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "minimal_graph = convert_variables_to_constants(session, session.graph_def, [\"y_pred_cls\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./graphs/tinyNet.pb'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.write_graph(minimal_graph, './graphs', 'tinyNet.pb', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_filepath(filepath):\n",
    "    count = 0\n",
    "    new_path = ''\n",
    "    for i in reversed(filepath):\n",
    "        if i == '/':\n",
    "            count += 1\n",
    "            if count == 2:\n",
    "                break\n",
    "        new_path += i    \n",
    "    new_path = new_path[::-1]\n",
    "    return new_path[:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_floats(images_path, save_dir='./floating_signatures/'):\n",
    "    for subdir, dirs, files in os.walk(images_path):\n",
    "        for file in files:\n",
    "            filepath = subdir + os.sep + file\n",
    "            if filepath.endswith(\".jpg\"):\n",
    "                img = imread(filepath)\n",
    "                if len(img.shape) != 3:\n",
    "                    continue\n",
    "                img = imresize(img, [48, 48])\n",
    "                values = session.run(fc1, feed_dict={x: [img]})\n",
    "                values = np.squeeze(values)\n",
    "                save_path1 = get_filepath(filepath)\n",
    "                check_folder = save_dir + save_path1[:3]\n",
    "                if not os.path.exists(check_folder):\n",
    "                    os.makedirs(check_folder)\n",
    "                save_path = os.path.join(save_dir, save_path1)\n",
    "                np.save(save_path, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_floats(images_path=images_path)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
